{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f900c2d-68d1-4e2f-a91e-acb043c99d32",
   "metadata": {},
   "source": [
    "# __Exploring Generative AI Libraries__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524fb28-3c27-4dd5-a838-ee9c93a16980",
   "metadata": {},
   "source": [
    "## What is generative AI?\n",
    "Imagine presenting a computer with a vast array of paintings. After analyzing them, it tries to craft a unique painting of its own. This capability is termed generative AI. Essentially, the computer derives inspiration from the provided content and uses it to create something new.\n",
    "\n",
    "## Real-world impact of generative AI\n",
    "Generative AI is transforming multiple industries. Its applications span:\n",
    "\n",
    "### 1. Art and creativity\n",
    "- Generative art: Artists employing generative AI algorithms can create stunning artworks by learning from existing masterpieces and producing unique pieces inspired by them. These AI-generated artworks have gained recognition in the art world.\n",
    "- Music Composition: Projects in the realm of generative AI have been employed to compose music. They learn from a vast data set of musical compositions and can generate original pieces in various styles, from classical to jazz, revolutionizing the music industry.\n",
    "\n",
    "### 2. Natural language processing (NLP)\n",
    "- Content generation: Tools like generative pre-trained transformer (GPT) have demonstrated their ability to generate coherent and context-aware text. They can assist content creators by generating articles, stories, or marketing copy, making them valuable tools in content creation.\n",
    "- Chatbots and virtual assistants: Generative AI powers many of today's chatbots and virtual assistants. These AI-driven conversational agents understand and generate human-like responses, enhancing user experiences.\n",
    "- Code Writing: Generative AI models can also produce code snippets based on descriptions or requirements, streamlining software development.\n",
    "\n",
    "### 3. Computer vision\n",
    "- Image synthesis: Models like data analysis learning with language model for generation and exploration, frequencly known as DALL-E, can generate images from textual descriptions. This technology finds applications in graphic design, advertising, and creating visual content for marketing.\n",
    "- Deepfake detection: With the advancement in generative AI techniques, the generation of deep fake content is also on the rise. Consequently, generative AI now plays a role in developing tools and techniques to detect and combat the spread of misinformation through manipulated videos.\n",
    "\n",
    "### 4. Virtual avatars\n",
    "- Entertainment: Generative AI is utilized to craft virtual avatars for gaming and entertainment. These avatars mimic human expressions and emotions, bolstering user engagement in virtual environments.\n",
    "- Marketing: Virtual influencers, propelled by generative AI, are on the rise in digital marketing. Brands are harnessing these virtual personas to endorse their products and services.\n",
    "\n",
    "## Neural structures behind generative AI\n",
    "Before we had the powerful transformers, which are like super-fast readers and understand lots of words at once, there were other methods used for making computers generate text. These methods were like the building blocks that led to the amazing capabilities we have today.\n",
    "\n",
    "## Large language models (LLMs)\n",
    "Large language models are like supercharged brains. They are massive computer programs with lots of \"neurons\" that learn from huge amounts of text. These models are trained to do tasks like understanding and generating text, and they're used in many applications. However, there's a limitation: these models are not very good at understanding the bigger context or the meaning of words. They work well for simple predictions but struggle with more complex text.\n",
    "\n",
    "## Text generation before transformers\n",
    "\n",
    "### 1. N-gram language models\n",
    "N-gram models are like language detectives. They predict what words come next in a sentence based on the words that came before. For example, if you say \"The sky is,\" these models guess that the next word might be \"blue.\"\n",
    "\n",
    "### 2. Recurrent neural networks (RNN)\n",
    "Recurrent neural networks (RNNs) are specially designed to handle sequential data, making them a powerful tool for applications like language modeling and time series forecasting. The essence of their design lies in maintaining a 'memory' or 'hidden state' throughout the sequence by employing loops. This enables RNNs to recognize and capture the temporal dependencies inherent in sequential data.\n",
    "- Hidden state: Often referred to as the network's 'memory', the hidden state is a dynamic storage of information about previous sequence inputs. With each new input, this hidden state is updated, factoring in both the new input and its previous value.\n",
    "- Temporal dependency: Loops in RNNs enable information transfer across sequence steps.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"60%\" height=\"60%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Image Source</a></div>\n",
    "\n",
    "Illustration of RNN's operation: Consider a simple sequence, such as the sentence: \"I love RNNs\". The RNN interprets this sentence word by word. Beginning with the word \"I\", the RNN ingests it, generates an output, and updates its hidden state. Moving on to \"love\", the RNN processes it alongside the updated hidden state which already holds insights about the word \"I\". The hidden state is updated again post this. This pattern of processing and updating continues until the last word. By the end of the sequence, the hidden state ideally encapsulates insights from the entire sentence.\n",
    "                                                                                                       \n",
    "### 3. Long short-term memory (LSTM) and gated recurrent units (GRUs)\n",
    "Long short-term memory (LSTM) and gated recurrent units (GRUs) are advanced variations of recurrent neural networks (RNNs), designed to address the limitations of traditional RNNs and enhance their ability to model sequential data effectively. They processed sequences one element at a time and maintained an internal state to remember past elements. While they were effective for a variety of tasks, they struggled with long sequences and long-term dependencies.\n",
    "\n",
    "### 4. Seq2seq models with attention\n",
    "- Sequence-to-sequence (seq2seq) models, often built with RNNs or LSTMs, were designed to handle tasks like translation where an input sequence is transformed into an output sequence.\n",
    "- The attention mechanism was introduced to allow the model to \"focus\" on relevant parts of the input sequence when generating the output, significantly improving performance on tasks like machine translation.\n",
    "\n",
    "While these methods provided significant advancements in text generation tasks, the introduction of transformers led to a paradigm shift. Transformers, with their self-attention mechanism, proved to be highly efficient at capturing contextual information across long sequences, setting new benchmarks in various NLP tasks.\n",
    "\n",
    "## Transformers\n",
    "Proposed in a paper titled \"Attention Is All You Need\" by Vaswani et al. in 2017, the transformer architecture replaced sequential processing with parallel processing. The key component behind its success? The attention mechanism, more precisely, self-attention.\n",
    "\n",
    "Key steps include:\n",
    "- Tokenization: The first step is breaking down a sentence into tokens (words or subwords).\n",
    "- Embedding: Each token is represented as a vector, capturing its meaning.\n",
    "- Self-attention: The model computes scores determining the importance of every other word for a particular word in the sequence. These scores are used to weight the input tokens and produce a new representation of the sequence. For instance, in the sentence \"He gave her a gift because she'd helped him\", understanding who \"her\" refers to requires the model to pay attention to other words in the sentence. The transformer does this for every word, considering the entire context, which is particularly powerful for understanding meaning.\n",
    "- Feed-forward neural networks: After attention, each position is passed through a feed-forward network separately.\n",
    "- Output sequence: The model produces an output sequence, which can be used for various tasks, like classification, translation, or text generation.\n",
    "- Layering: Importantly, transformers are deep models with multiple layers of attention and feed-forward networks, allowing them to learn complex patterns.\n",
    "\n",
    "The architecture's flexibility has allowed transformers to be used beyond NLP, finding applications in image and video processing too. In NLP, transformer-based models like BERT, GPT, and their variants have set state-of-the-art results in various tasks, from text classification to translation.\n",
    "\n",
    "### Implementation: Building a simple chatbot with transformers\n",
    "Now, you will build a simple chatbot using `transformers` library from Hugging Face, which is an open-source natural language processing (NLP) toolkit with many useful features.\n",
    "#### Step 1: Installing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd56ae9e-66c4-4e0c-9e5c-74f52d3157be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\nensi\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (2022.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.17.2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (2.28.1)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (1.26.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.17.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (1.26.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq tensorflow\n",
    "!pip install -qq transformers\n",
    "!pip install sentencepiece\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26\n",
    "#!pip install --upgrade numpy transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\nensi\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in c:\\users\\nensi\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: requests in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nensi\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in c:\\users\\nensi\\anaconda3\\lib\\site-packages (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)'))': /simple/certifi/\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\nensi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nensi\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# import requests\n",
    "# requests.get(\"https://example.com\", verify=False)\n",
    "\n",
    "# pip install certifi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c08a0d-c879-44a8-8a97-0166dc52e22e",
   "metadata": {},
   "source": [
    "#### Step 2: Importing the required tools from the transformers library\n",
    "In the upcoming script, you initiate variables using two invaluable classes from the transformers library:\n",
    "- `model` is an instance of the class `AutoModelForSeq2SeqLM`. This class lets you interact with your chosen language model.\n",
    "- `tokenizer` is an instance of the class `AutoTokenizer`. This class streamlines your input and presents it to the language model in the most efficient manner. It achieves this by converting your text input into \"tokens\", which is the model's preferred way of interpreting text.\n",
    "You choose \"facebook/blenderbot-400M-distill\" for this example model because it is freely available under an open-source license and operates at a relatively brisk pace. For a diverse range of models and their capabilities, you can explore the Hugging Face website: [Hugging Face Models](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07610831-7e22-4f70-b90a-8c6631764477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f44c27-951f-4667-9d51-0aac4daaad81",
   "metadata": {},
   "source": [
    "Following the initialization, let's set up the chat function to enable real-time interaction with the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e74bcc-9f42-4f5e-bb9e-4e25d2d62ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: nensi\n",
      "Chatbot: I am a member of the National Association for the Prevention of Cruelty to Animals.\n",
      "You: nice\n",
      "Chatbot: Thank you.  I am so happy.  It's been a long time since I've been this happy.\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310336f-8e60-4127-a2bd-874d092e9a8b",
   "metadata": {},
   "source": [
    "#### Step 3: Trying another language model and comparing the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca811dcb-696d-4563-a576-af4b9f767505",
   "metadata": {},
   "source": [
    "You can use a different language model, for example the \"[flan-t5-base](https://huggingface.co/google/flan-t5-base)\" model from Google, to create a similar chatbot. You can use a chat function similar to the one defined in Step 2 and compare the outputs of both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00536198-970f-4d2a-9207-ef0e8f848cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e155437ff32d45b39ac437b6e30395ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nensi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nensi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9068cfc4-2c17-4c14-a850-e8c2b7bbd5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: I am Nensi\n",
      "Chatbot: I am Nensi\n",
      "You: How are you\n",
      "Chatbot: I'm a little bit nervous.\n",
      "You: Why?\n",
      "Chatbot: a symphony orchestra\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "### Let's chat with another bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28014317-f246-4dcd-a4a3-a4cc11e27039",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad5172-3dc8-4ee1-8d37-6b00beb3e545",
   "metadata": {},
   "source": [
    "### Create a chatbot using different models from Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b759-9c83-44b9-8629-b0a5e84e58af",
   "metadata": {},
   "source": [
    "Create a simple chatbot using the transformers library from Hugging Face(https://huggingface.co/models). Run the code using the following models and compare the output. The models are \"[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\", \"[facebook/bart-base](https://huggingface.co/facebook/bart-base)\". \n",
    "(Note: Based on the selected model, you may notice differences in the chatbot output. Multiple factors, such as model training and fine-tuning, influence the output.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8094006b-c658-4d83-89a1-076b6e7ccbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf60360507d04d65aa564878c3aecd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35db89db59e497c8d8811b025b7aa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea4d27640b344cdba917c3692859956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49e5bbdc3524e2a98a5c01f9af82980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b326a5dd40604a44ac46b1a647dd4053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\" #here the model name can be changed as you like.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Nensi here\n",
      "Chatbot: Nensi\n",
      "You: How was your day\n",
      "Chatbot: i was a little bit sluggish\n",
      "You: what happened \n",
      "Chatbot: a syphilis was found in the syphilis\n",
      "You: bye\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "prev_pub_hash": "8f0dd01f11399ed415e65f476757f34cc91047b88b047a9c5865e31d35c1838f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
